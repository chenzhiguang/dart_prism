[
  {
    "consumers": [
      "graphql"
    ],
    "stringified": "[tokenize](code, grammar, Prism) {\n\t\t\tconst tokens = Prism.tokenize(code, withoutTokenize(grammar));\n\n\t\t\t/** @typedef {import(\"../core/token\").Token} Token */\n\n\t\t\t/**\n\t\t\t * @param {Token | string} token\n\t\t\t * @returns {token is Token}\n\t\t\t */\n\t\t\tfunction isToken(token) {\n\t\t\t\treturn typeof token !== 'string';\n\t\t\t}\n\n\t\t\t/**\n\t\t\t * get the graphql token stream that we want to customize\n\t\t\t */\n\t\t\tconst validTokens = tokens\n\t\t\t\t.filter(isToken)\n\t\t\t\t.filter((token) => token.type !== 'comment' && token.type !== 'scalar');\n\n\t\t\tlet currentIndex = 0;\n\n\t\t\t/**\n\t\t\t * Returns whether the token relative to the current index has the given type.\n\t\t\t *\n\t\t\t * @param {number} offset\n\t\t\t * @returns {Token}\n\t\t\t */\n\t\t\tfunction getToken(offset) {\n\t\t\t\treturn validTokens[currentIndex + offset];\n\t\t\t}\n\n\t\t\t/**\n\t\t\t * Returns whether the token relative to the current index has the given type.\n\t\t\t *\n\t\t\t * @param {readonly string[]} types\n\t\t\t * @param {number} [offset=0]\n\t\t\t * @returns {boolean}\n\t\t\t */\n\t\t\tfunction isTokenType(types, offset = 0) {\n\t\t\t\tfor (let i = 0; i < types.length; i++) {\n\t\t\t\t\tconst token = getToken(i + offset);\n\t\t\t\t\tif (!token || token.type !== types[i]) {\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn true;\n\t\t\t}\n\n\t\t\t/**\n\t\t\t * Returns the index of the closing bracket to an opening bracket.\n\t\t\t *\n\t\t\t * It is assumed that `token[currentIndex - 1]` is an opening bracket.\n\t\t\t *\n\t\t\t * If no closing bracket could be found, `-1` will be returned.\n\t\t\t *\n\t\t\t * @param {RegExp} open\n\t\t\t * @param {RegExp} close\n\t\t\t * @returns {number}\n\t\t\t */\n\t\t\tfunction findClosingBracket(open, close) {\n\t\t\t\tlet stackHeight = 1;\n\n\t\t\t\tfor (let i = currentIndex; i < validTokens.length; i++) {\n\t\t\t\t\tconst token = validTokens[i];\n\t\t\t\t\tconst content = token.content;\n\n\t\t\t\t\tif (token.type === 'punctuation' && typeof content === 'string') {\n\t\t\t\t\t\tif (open.test(content)) {\n\t\t\t\t\t\t\tstackHeight++;\n\t\t\t\t\t\t} else if (close.test(content)) {\n\t\t\t\t\t\t\tstackHeight--;\n\n\t\t\t\t\t\t\tif (stackHeight === 0) {\n\t\t\t\t\t\t\t\treturn i;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tfor (; currentIndex < validTokens.length;) {\n\t\t\t\tconst startToken = validTokens[currentIndex++];\n\n\t\t\t\t// add special aliases for mutation tokens\n\t\t\t\tif (startToken.type === 'keyword' && startToken.content === 'mutation') {\n\t\t\t\t\t// any array of the names of all input variables (if any)\n\t\t\t\t\tconst inputVariables = [];\n\n\t\t\t\t\tif (isTokenType(['definition-mutation', 'punctuation']) && getToken(1).content === '(') {\n\t\t\t\t\t\t// definition\n\n\t\t\t\t\t\tcurrentIndex += 2; // skip 'definition-mutation' and 'punctuation'\n\n\t\t\t\t\t\tconst definitionEnd = findClosingBracket(/^\\($/, /^\\)$/);\n\t\t\t\t\t\tif (definitionEnd === -1) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// find all input variables\n\t\t\t\t\t\tfor (; currentIndex < definitionEnd; currentIndex++) {\n\t\t\t\t\t\t\tconst t = getToken(0);\n\t\t\t\t\t\t\tif (t.type === 'variable') {\n\t\t\t\t\t\t\t\tt.addAlias('variable-input');\n\t\t\t\t\t\t\t\tinputVariables.push(t.content);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tcurrentIndex = definitionEnd + 1;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (isTokenType(['punctuation', 'property-query']) && getToken(0).content === '{') {\n\t\t\t\t\t\tcurrentIndex++; // skip opening bracket\n\n\t\t\t\t\t\tgetToken(0).addAlias('property-mutation');\n\n\t\t\t\t\t\tif (inputVariables.length > 0) {\n\t\t\t\t\t\t\tconst mutationEnd = findClosingBracket(/^\\{$/, /^\\}$/);\n\t\t\t\t\t\t\tif (mutationEnd === -1) {\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// give references to input variables a special alias\n\t\t\t\t\t\t\tfor (let i = currentIndex; i < mutationEnd; i++) {\n\t\t\t\t\t\t\t\tconst varToken = validTokens[i];\n\t\t\t\t\t\t\t\tif (varToken.type === 'variable' && inputVariables.includes(varToken.content)) {\n\t\t\t\t\t\t\t\t\tvarToken.addAlias('variable-input');\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn tokens;\n\t\t}"
  },
  {
    "consumers": [
      "jsx",
      "tsx",
      "xquery"
    ],
    "stringified": "(code, grammar, Prism) => {\n\t\t\tconst tokens = Prism.tokenize(code, withoutTokenize(grammar));\n\t\t\twalkTokens(tokens);\n\t\t\treturn tokens;\n\t\t}"
  },
  {
    "consumers": [
      "markdown"
    ],
    "stringified": "[tokenize](code, grammar, Prism) {\n\t\t\t\t\t\t\tconst tokens = Prism.tokenize(code, withoutTokenize(grammar));\n\n\t\t\t\t\t\t\t/*\n\t\t\t\t\t\t\t * Add the correct `language-xxxx` class to this code block. Keep in mind that the `code-language` token\n\t\t\t\t\t\t\t * is optional. But the grammar is defined so that there is only one case we have to handle:\n\t\t\t\t\t\t\t *\n\t\t\t\t\t\t\t * token.content = [\n\t\t\t\t\t\t\t *     <span class=\"punctuation\">```</span>,\n\t\t\t\t\t\t\t *     <span class=\"code-language\">xxxx</span>,\n\t\t\t\t\t\t\t *     '\\n', // exactly one new lines (\\r or \\n or \\r\\n)\n\t\t\t\t\t\t\t *     <span class=\"code-block\">...</span>,\n\t\t\t\t\t\t\t *     '\\n', // exactly one new lines again\n\t\t\t\t\t\t\t *     <span class=\"punctuation\">```</span>\n\t\t\t\t\t\t\t * ];\n\t\t\t\t\t\t\t */\n\n\t\t\t\t\t\t\tconst codeLang = tokens[1];\n\t\t\t\t\t\t\tconst codeBlock = tokens[3];\n\n\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\ttypeof codeLang === 'object' && typeof codeBlock === 'object' &&\n\t\t\t\t\t\t\t\tcodeLang.type === 'code-language' && codeBlock.type === 'code-block'\n\t\t\t\t\t\t\t) {\n\n\t\t\t\t\t\t\t\t// this might be a language that Prism does not support\n\n\t\t\t\t\t\t\t\t// do some replacements to support C++, C#, and F#\n\t\t\t\t\t\t\t\tconst lang = getTextContent(codeLang.content)\n\t\t\t\t\t\t\t\t\t.replace(/\\b#/g, 'sharp')\n\t\t\t\t\t\t\t\t\t.replace(/\\b\\+\\+/g, 'pp');\n\t\t\t\t\t\t\t\t// only use the first word\n\t\t\t\t\t\t\t\tconst langName = /[a-z][\\w-]*/i.exec(lang)?.[0].toLowerCase();\n\t\t\t\t\t\t\t\tif (langName) {\n\t\t\t\t\t\t\t\t\tcodeBlock.addAlias('language-' + langName);\n\n\t\t\t\t\t\t\t\t\tconst grammar = Prism.components.getLanguage(lang);\n\t\t\t\t\t\t\t\t\tif (grammar) {\n\t\t\t\t\t\t\t\t\t\tcodeBlock.content = Prism.tokenize(getTextContent(codeBlock), grammar);\n\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\tcodeBlock.addAlias('needs-highlighting');\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\treturn tokens;\n\t\t\t\t\t\t}"
  },
  {
    "consumers": [
      "naniscript"
    ],
    "stringified": "[tokenize](code, grammar, Prism) {\n\t\t\t\tconst tokens = Prism.tokenize(code, withoutTokenize(grammar));\n\t\t\t\ttokens.forEach((token) => {\n\t\t\t\t\tif (typeof token !== 'string' && token.type === 'generic-text') {\n\t\t\t\t\t\tconst content = getTextContent(token);\n\t\t\t\t\t\tif (!isBracketsBalanced(content)) {\n\t\t\t\t\t\t\ttoken.type = 'bad-line';\n\t\t\t\t\t\t\ttoken.content = content;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\treturn tokens;\n\t\t\t}"
  },
  {
    "consumers": [
      "php"
    ],
    "stringified": "(code, grammar, Prism) => {\n\t\t\t\tif (!/<\\?/.test(code)) {\n\t\t\t\t\treturn Prism.tokenize(code, php);\n\t\t\t\t}\n\n\t\t\t\treturn embedded(code, grammar, Prism);\n\t\t\t}"
  },
  {
    "consumers": [
      "treeview"
    ],
    "stringified": "[tokenize](code, grammar, Prism) {\n\t\t\t\t\tconst tokens = Prism.tokenize(code, withoutTokenize(grammar));\n\n\t\t\t\t\tfor (const token of tokens) {\n\t\t\t\t\t\tif (typeof token === 'string' || token.type !== 'entry-name' || typeof token.content === 'string') {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (token.content.some((t) => typeof t !== 'string' && t.type === 'symlink')) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tconst text = getTextContent(token);\n\n\t\t\t\t\t\tconst folderPattern = /(?:^|[^\\\\])\\/$/;\n\t\t\t\t\t\tif (folderPattern.test(text)) {\n\t\t\t\t\t\t\t// folder\n\t\t\t\t\t\t\ttoken.addAlias('dir');\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// file\n\n\t\t\t\t\t\t\tconst parts = text\n\t\t\t\t\t\t\t\t.replace(/(^|[^\\\\])[=*|]$/, '$1')\n\t\t\t\t\t\t\t\t.toLowerCase()\n\t\t\t\t\t\t\t\t.replace(/\\s+/g, '')\n\t\t\t\t\t\t\t\t.split('.');\n\n\t\t\t\t\t\t\twhile (parts.length > 1) {\n\t\t\t\t\t\t\t\tparts.shift();\n\t\t\t\t\t\t\t\t// Ex. 'foo.min.js' would become '<span class=\"token keyword ext-min-js ext-js\">foo.min.js</span>'\n\t\t\t\t\t\t\t\ttoken.addAlias('ext-' + parts.join('-'));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (text.startsWith('.')) {\n\t\t\t\t\t\t\ttoken.addAlias('dotfile');\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\treturn tokens;\n\t\t\t\t}"
  }
]